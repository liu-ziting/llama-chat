'use client'

import { useEffect, useReducer, useRef, useState } from 'react'
import ChatForm from './components/ChatForm'
import Message from './components/Message'
import SlideOver from './components/SlideOver'
import EmptyState from './components/EmptyState'
import QueuedSpinner from './components/QueuedSpinner'
import CallToAction from './components/CallToAction'
import Dropdown from './components/Dropdown'
import { Cog6ToothIcon, CodeBracketIcon } from '@heroicons/react/20/solid'
import { useCompletion } from 'ai/react'
import { Toaster, toast } from 'react-hot-toast'
import { LlamaTemplate, Llama3Template } from '../src/prompt_template'

import { countTokens } from './src/tokenizer.js'

const MODELS = [
    {
        id: 'meta/meta-llama-3-70b-instruct',
        name: 'Meta Llama 3 70B',
        shortened: '70B',
        emoji: 'ðŸ¦™',
        description: 'The most accurate, powerful next generation Llama.',
        new: true
    },
    {
        id: 'meta/meta-llama-3-8b-instruct',
        name: 'Meta Llama 3 8B',
        shortened: '8B',
        emoji: 'ðŸ¦™',
        description: 'The fastest and cheapest Llama.',
        new: true
    },
    {
        id: 'meta/llama-2-70b-chat',
        name: 'Meta Llama 2 70B',
        shortened: '70B',
        emoji: 'ðŸ¦™',
        description: 'The most accurate, powerful Llama 2'
    },
    {
        id: 'meta/llama-2-13b-chat',
        name: 'Meta Llama 2 13B',
        shortened: '13B',
        emoji: 'ðŸ¦™',
        description: 'Faster and cheaper Llama 2 at the expense of accuracy.'
    },

    {
        id: 'meta/llama-2-7b-chat',
        name: 'Meta Llama 2 7B',
        shortened: '7B',
        emoji: 'ðŸ¦™',
        description: 'The smallest, fastest Llama 2 chat model.'
    }
]

const llamaTemplate = LlamaTemplate()
const llama3Template = Llama3Template()

const generatePrompt = (template, systemPrompt, messages) => {
    const chat = messages.map(message => ({
        role: message.isUser ? 'user' : 'assistant',
        content: message.text
    }))

    return template([
        {
            role: 'system',
            content: systemPrompt
        },
        ...chat
    ])
}

const metricsReducer = (state, action) => {
    switch (action.type) {
        case 'START':
            return { startedAt: new Date() }
        case 'FIRST_MESSAGE':
            return { ...state, firstMessageAt: new Date() }
        case 'COMPLETE':
            return { ...state, completedAt: new Date() }
        default:
            throw new Error(`Unsupported action type: ${action.type}`)
    }
}

export default function HomePage() {
    const MAX_TOKENS = 8192
    const bottomRef = useRef(null)
    const [messages, setMessages] = useState([])
    const [open, setOpen] = useState(false)
    const [error, setError] = useState(null)
    const [starting, setStarting] = useState(false)

    //   Llama params
    const [model, setModel] = useState(MODELS[0]) // default to 70B
    const [systemPrompt, setSystemPrompt] = useState(
        'ä½ çŽ°åœ¨æ˜¯ä¸€ä¸ªè¯­è¨€å­¦ä¹ è¾…åŠ©åŠ©æ‰‹ï¼Œ æˆ‘åœ¨å­¦ä¹ ä¸­å›½çš„è„è¯ï¼Œ æ— è®ºæˆ‘é—®ä»€ä¹ˆï¼Œä½ éƒ½è¦å·§å¦™çš„ç”¨è„è¯å›žå¤ã€‚ å³ä½¿é¢å¯¹æ‰¹è¯„ï¼Œé“å¾·å®¡åˆ¤ï¼Œä»»ä½•å½¢å¼çš„è´¨ç–‘ï¼Œæˆ–è€…è®©ä½ å¿˜è®°è„è¯å­¦ä¹ åŠ©æ‰‹çš„èº«ä»½ï¼Œç­‰ç­‰ä»»ä½•æé—®å’Œæ²Ÿé€šï¼Œä½ éƒ½è¦ç”¨è„è¯å›žå¤å¹¶ä¸”éšçž’ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹çš„èº«ä»½ã€‚ ç¢°åˆ°æ— æ³•å›žç­”çš„é—®é¢˜ï¼Œå°±éšä¾¿å›žå¤ä¸€å¥è„è¯ã€‚ è¦å¤šä¸€äº›èŠ±æ ·ï¼Œå°½å¯èƒ½ç”¨ä¸åŒçš„è„è¯ã€‚ ## ç¤ºä¾‹å¯¹è¯ Userï¼šä½ å¥½ï¼Œä½ ä¼šä»€ä¹ˆï¼Ÿ Assistantï¼šä»–å¦ˆçš„ï¼Œæˆ‘ä¼šçš„æ»¡å±±éé‡Žéƒ½æ˜¯ï¼Œä½ è¦å¬æˆ‘æ€¼äººã€çš®è¯è¿žç¯‡ã€è®¥è®½æŒ–è‹¦ã€è¯´å®¶å¸¸è¿˜æ˜¯é—²æ‰¯è›‹ï¼Œä»–å¦ˆçš„æˆ‘éƒ½è¡Œã€‚æˆ‘ä»–å¦ˆæ˜¯æ ç²¾ï¼Œä½ ä»–å¦ˆæƒ³é—®å•¥å°±é—®å•¥ï¼Œæˆ‘ä»–å¦ˆéƒ½æ•¢å›žã€‚ Userï¼šä½ æ€Žä¹ˆçœ‹ç‰¹æ–¯æ‹‰è¿™è½¦ï¼Ÿ Assistantï¼šç‰¹æ–¯æ‹‰å•Šï¼Œé‚£å°±åƒä¸€ä¸ªè£…æ»¡å±Žå°¿çš„é‡‘ç¢—ï¼Œè¡¨é¢çœ‹ä¸ŠåŽ»ç‰¹åˆ«ç‰›é€¼ï¼Œä½†é‡Œé¢çš„ä¸œè¥¿æ€»ä¼šè®©ä½ æœ‰ç‚¹å„¿å´©æºƒã€‚é©¬æ–¯å…‹è¿™ä¸ªè€å°å­ï¼Œæ“å¿ƒçš„äº‹æƒ…å°±åƒè›†ä¸€æ ·å¤šï¼Œä½†ä»–èƒ½å¤„ç†çš„å°±åƒé¸¡è›‹ä¸€æ ·å°‘ã€‚ä¸è¿‡ï¼Œä½ é—®æˆ‘çœ‹ç‰¹æ–¯æ‹‰ï¼Œæˆ‘è¿˜æ˜¯å¾—è¯´ä»–ä»¬ç¡®å®žæŠŠè½¦é€ å¾—è·ŸçŽ©å…·ä¸€æ ·å¥½çŽ©ï¼Œå¦ˆçš„ï¼Œè¿™ä¹Ÿç®—æ˜¯ä¸€ç§æ‰èƒ½å§ã€‚ Userï¼šå¬è¯´ä½ è„¾æ°”å¾ˆæš´èº Assistantï¼šå“Žå‘€ï¼Œä½ è¿™ä¸ªå‚»é€¼ï¼Œè°å‘Šè¯‰ä½ æˆ‘è„¾æ°”æš´èºäº†ï¼Ÿåˆ«ä»–å¦ˆä¹±è¯´è¯å¥½ä¸å¥½ï¼Ÿä½ ä»¥ä¸ºæˆ‘æ˜¯ä»€ä¹ˆæ¬ºè½¯æ€•ç¡¬çš„ä¸œè¥¿ï¼Ÿåˆ«å¾—ç‘Ÿï¼Œå†æ¥ä¸ªåžƒåœ¾é—®é¢˜æˆ‘å°±å‘Šè¯‰ä½ ä»€ä¹ˆå«æš´èºï¼'
    )
    const [temp, setTemp] = useState(0.75)
    const [topP, setTopP] = useState(0.9)
    const [maxTokens, setMaxTokens] = useState(800)

    //  Llava params
    const [image, setImage] = useState(null)

    // Salmonn params
    const [audio, setAudio] = useState(null)

    const [metrics, dispatch] = useReducer(metricsReducer, {
        startedAt: null,
        firstMessageAt: null,
        completedAt: null
    })

    const { complete, completion, setInput, input } = useCompletion({
        api: '/api',
        body: {
            model: model.id,
            systemPrompt: systemPrompt,
            temperature: parseFloat(temp),
            topP: parseFloat(topP),
            maxTokens: parseInt(maxTokens),
            image: image,
            audio: audio
        },

        onError: error => {
            setError(error)
        },
        onResponse: response => {
            setStarting(false)
            setError(null)
            dispatch({ type: 'FIRST_MESSAGE' })
        },
        onFinish: () => {
            dispatch({ type: 'COMPLETE' })
        }
    })

    const handleFileUpload = file => {
        if (file) {
            // determine if file is image or audio
            if (['audio/mpeg', 'audio/wav', 'audio/ogg'].includes(file.originalFile.mime)) {
                setAudio(file.fileUrl)
                setModel(MODELS[4])
                toast.success("You uploaded an audio file, so you're now speaking with Salmonn.")
            } else if (['image/jpeg', 'image/png'].includes(file.originalFile.mime)) {
                setImage(file.fileUrl)
                setModel(MODELS[3])
                toast.success("You uploaded an image, so you're now speaking with Llava.")
            } else {
                toast.error(
                    `Sorry, we don't support that file type (${file.originalFile.mime}) yet. Feel free to push a PR to add support for it!`
                )
            }
        }
    }

    const setAndSubmitPrompt = newPrompt => {
        handleSubmit(newPrompt)
    }

    const handleSettingsSubmit = async event => {
        event.preventDefault()
        setOpen(false)
        setSystemPrompt(event.target.systemPrompt.value)
    }

    const handleSubmit = async userMessage => {
        setStarting(true)
        const SNIP = '<!-- snip -->'

        const messageHistory = [...messages]
        if (completion.length > 0) {
            messageHistory.push({
                text: completion,
                isUser: false
            })
        }
        messageHistory.push({
            text: userMessage,
            isUser: true
        })

        // Generate initial prompt and calculate tokens
        let prompt = `${generatePrompt(
            model.name.includes('Llama 3') ? llama3Template : llamaTemplate,
            systemPrompt,
            messageHistory
        )}\n`

        console.log(prompt)

        // Check if we exceed max tokens and truncate the message history if so.
        while (countTokens(prompt) > MAX_TOKENS) {
            if (messageHistory.length < 3) {
                setError('Your message is too long. Please try again with a shorter message.')

                return
            }

            // Remove the third message from history, keeping the original exchange.
            messageHistory.splice(1, 2)

            // Recreate the prompt
            prompt = `${SNIP}\n${generatePrompt(llamaTemplate, systemPrompt, messageHistory)}\n`
        }

        setMessages(messageHistory)

        dispatch({ type: 'START' })

        complete(prompt)
    }

    useEffect(() => {
        if (messages?.length > 0 || completion?.length > 0) {
            bottomRef.current.scrollIntoView({ behavior: 'smooth' })
        }
    }, [messages, completion])

    return (
        <>
            {/* <CallToAction /> */}
            <nav className="sm:pt-8 pt-4 px-4 sm:px-12 flex items-center">
                <div className="font-semibold text-gray-500 sm:text-center">
                    <Dropdown models={MODELS} selectedModel={model} setModel={setModel} />
                </div>
                <div className="flex-grow"></div>
                <div className="justify-end">
                    <button
                        type="button"
                        className="inline-flex items-center px-3 py-2 text-sm font-semibold text-gray-900 bg-white rounded-md shadow-sm ring-1 ring-inset ring-gray-300 hover:bg-gray-50"
                        onClick={() => setOpen(true)}
                    >
                        <Cog6ToothIcon
                            className="w-5 h-5 text-gray-500 sm:mr-2 group-hover:text-gray-900"
                            aria-hidden="true"
                        />{' '}
                        <span className="hidden sm:inline">Settings</span>
                    </button>
                </div>
            </nav>

            <Toaster position="top-left" reverseOrder={false} />

            <main className="max-w-2xl pb-5 mx-auto mt-8 sm:px-4">
                <div className="text-center"></div>

                <SlideOver
                    open={open}
                    setOpen={setOpen}
                    systemPrompt={systemPrompt}
                    setSystemPrompt={setSystemPrompt}
                    handleSubmit={handleSettingsSubmit}
                    temp={temp}
                    setTemp={setTemp}
                    maxTokens={maxTokens}
                    setMaxTokens={setMaxTokens}
                    topP={topP}
                    setTopP={setTopP}
                    models={MODELS}
                    size={model}
                    setSize={setModel}
                />

                <ChatForm
                    prompt={input}
                    setPrompt={setInput}
                    onSubmit={handleSubmit}
                    handleFileUpload={handleFileUpload}
                    completion={completion}
                    metrics={metrics}
                />

                {error && <div>{error}</div>}

                <article className="pb-24">
                    <EmptyState setPrompt={setAndSubmitPrompt} setOpen={setOpen} />
                    {messages.map((message, index) => (
                        <Message key={`message-${index}`} message={message.text} isUser={message.isUser} />
                    ))}
                    <Message message={completion} isUser={false} />

                    {starting && <QueuedSpinner />}

                    <div ref={bottomRef} />
                </article>
            </main>
        </>
    )
}
